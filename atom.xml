<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://adiaosdu.github.io</id>
    <title>Diao&apos;s Domain</title>
    <updated>2022-03-17T03:05:59.146Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://adiaosdu.github.io"/>
    <link rel="self" href="https://adiaosdu.github.io/atom.xml"/>
    <logo>https://adiaosdu.github.io/images/avatar.png</logo>
    <icon>https://adiaosdu.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, Diao&apos;s Domain</rights>
    <entry>
        <title type="html"><![CDATA[LDA主题挖掘模型]]></title>
        <id>https://adiaosdu.github.io/post/PNVcBHgYq/</id>
        <link href="https://adiaosdu.github.io/post/PNVcBHgYq/">
        </link>
        <updated>2022-03-16T12:58:43.000Z</updated>
        <content type="html"><![CDATA[<p>由 Blei等人于2003年提出，用于推测文档的主题分布。它可以将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题分布后，便可以根据主题分布进行主题聚类或文本分类等任务。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程——特征选择]]></title>
        <id>https://adiaosdu.github.io/post/8aQ9H2hSe/</id>
        <link href="https://adiaosdu.github.io/post/8aQ9H2hSe/">
        </link>
        <updated>2022-03-16T01:32:22.000Z</updated>
        <content type="html"><![CDATA[<p>定义：从给定的特征集合中选出相关特征子集的过程称为特征选择(feature selection)。</p>
<ol>
<li>对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说很关键，但有些属性意义就不大。</li>
</ol>
<ul>
<li>对当前学习任务有用的属性或者特征，称为相关特征(relevant feature)；</li>
<li>对当前学习任务没用的属性或者特征，称为无关特征(irrelevant feature)。</li>
</ul>
<ol start="2">
<li>特征选择可能会降低模型的预测能力，因为被剔除的特征中可能包含了有效的信息，抛弃这部分信息一定程度上会降低模型的性能。但这也是计算复杂度和模型性能之间的取舍：</li>
</ol>
<ul>
<li>如果保留尽可能多的特征，模型的性能会提升，但同时模型就变复杂，计算复杂度也同样提升；</li>
<li>如果剔除尽可能多的特征，模型的性能会有所下降，但模型就变简单，也就降低计算复杂度。</li>
</ul>
<ol start="3">
<li>常见的特征选择方法分为以下三种，主要区别在于特征选择部分是否使用后续的学习器。</li>
</ol>
<ul>
<li>过滤式(filter)：先对数据集进行特征选择，其过程与后续学习器无关，即设计一些统计量来过滤特征，并不考虑后续学习器问题</li>
<li>包裹式(wrapper)：实际上就是一个分类器，它是将后续的学习器的性能作为特征子集的评价标准。</li>
<li>嵌入式(embedding)：实际上是学习器自主选择特征。</li>
</ul>
<h3 id="特征选择原理">特征选择原理</h3>
<ol>
<li>
<p>采用特征选择的原因：<br>
维数灾难问题。因为属性或者特征过多造成的问题，如果可以选择重要的特征，使得仅需要一部分特征就可以构建模型，可以大大减轻维数灾难问题，从这个意义上讲，特征选择和降维技术有相似的动机，事实上它们也是处理高维数据的两大主流技术。去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度。</p>
</li>
<li>
<p>特征选择最重要的是确保不丢失重要的特征，否则就会因为缺少重要的信息而无法得到一个性能很好的模型。</p>
</li>
</ol>
<p>给定数据集，学习任务不同，相关的特征很可能也不相同，因此特征选择中的不相关特征指的是与当前学习任务无关的特征。<br>
有一类特征称作冗余特征(redundant feature)，它们所包含的信息可以从其他特征中推演出来。<br>
冗余特征通常都不起作用，去除它们可以减轻模型训练的负担；<br>
但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则它是有益的，可以降低学习任务的难度。</p>
<ol start="3">
<li>在没有任何先验知识，即领域知识的前提下，要想从初始特征集合中选择一个包含所有重要信息的特征子集，唯一做法就是遍历所有可能的特征组合。</li>
</ol>
<p>但这种做法并不实际，也不可行，因为会遭遇组合爆炸，特征数量稍多就无法进行。</p>
<p>一个可选的方案是：</p>
<ul>
<li>产生一个候选子集，评价出它的好坏。</li>
<li>基于评价结果产生下一个候选子集，再评价其好坏。</li>
<li>这个过程持续进行下去，直至无法找到更好的后续子集为止。</li>
</ul>
<p>这里有两个问题：</p>
<ul>
<li>如何根据评价结果获取下一个候选特征子集？</li>
<li>如何评价候选特征子集的好坏？</li>
</ul>
<h4 id="子集搜索">子集搜索</h4>
<p>子集搜索方法步骤如下：</p>
<p>给定特征集合 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mo>=</mo><mrow><msub><mi>A</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>A</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>A</mi><mi>d</mi></msub></mrow></mrow><annotation encoding="application/x-tex">A={A_1,A_2,…,A_d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> ，首先将每个特征看作一个候选子集（即每个子集中只有一个元素），然后对这 d 个候选子集进行评价。<br>
假设 A2 最优，于是将 A2 作为第一轮的选定子集。<br>
然后在上一轮的选定子集中加入一个特征，构成了包含两个特征的候选子集。<br>
假定 A2,A5 最优，且优于 A2 ，于是将 A2,A5 作为第二轮的选定子集。<br>
......<br>
假定在第 k+1 轮时，本轮的最优的特征子集不如上一轮的最优的特征子集，则停止生成候选子集，并将上一轮选定的特征子集作为特征选择的结果。</p>
<p>这种逐渐增加相关特征的策略称作前向 forward搜索</p>
<p>类似地，如果从完整的特征集合开始，每次尝试去掉一个无关特征，这种逐渐减小特征的策略称作后向backward搜索</p>
<p>也可以将前向和后向搜索结合起来，每一轮逐渐增加选定的相关特征（这些特征在后续迭代中确定不会被去除），同时减少无关特征，这样的策略被称作是双向bidirectional搜索。</p>
<p>该策略是贪心的，因为它们仅仅考虑了使本轮选定集最优。但是除非进行穷举搜索，否则这样的问题无法避免。</p>
<h4 id="子集评价">子集评价</h4>
<p>子集评价的做法如下：给定数据集 D，假设所有属性均为离散型。对属性子集 A，假定根据其取值将 D 分成了 V 个子集。计算属性子集A的信息增益。信息增益越大，表明特征子集 A 包含的有助于分类的信息越多。所以对于每个候选特征子集，可以基于训练集 D 来计算其信息增益作为评价准则。</p>
<p>更一般地，特征子集 A 实际上确定了对数据集 D 的一个划分规则。<br>
每个划分区域对应着 A 上的一个取值，而样本标记信息 y 则对应着 D 的真实划分。<br>
通过估算这两种划分之间的差异，就能对 A 进行评价：与 y 对应的划分的差异越小，则说明 A 越好。<br>
信息熵仅仅是判断这个差异的一种方法，其他能判断这两个划分差异的机制都能够用于特征子集的评价。</p>
<p>将特征子集搜索机制与子集评价机制结合就能得到特征选择方法。<br>
事实上，决策树可以用于特征选择，所有树结点的划分属性所组成的集合就是选择出来的特征子集。<br>
其他特征选择方法本质上都是显式或者隐式地结合了某些子集搜索机制和子集评价机制。</p>
<p>最简单的特征选择方法是：去掉取值变化小的特征。假如某特征只有 0 和 1 的两种取值，并且所有输入样本中，95% 的样本的该特征取值都是 1 ，那就可以认为该特征作用不大。</p>
<p>当然，该方法的一个前提是，特征值都是离散型才使用该方法；如果是连续型，需要离散化后再使用，并且实际上一般不会出现 95% 以上都取某个值的特征的存在。</p>
<p>所以，这个方法简单，但不太好用，可以作为特征选择的一个预处理，先去掉变化小的特征，然后再开始选择上述三种类型的特征选择方法。</p>
<h3 id="过滤式选择">过滤式选择</h3>
<p>该方法先对数据集进行特征选择，然后再训练学习器。特征选择过程与后续学习器无关。<br>
也就是先采用特征选择对初始特征进行过滤，然后用过滤后的特征训练模型。<br>
优点是计算时间上比较高效，而且对过拟合问题有较高的鲁棒性；<br>
缺点是倾向于选择冗余特征，即没有考虑到特征之间的相关性。</p>
<h4 id="relief-方法">Relief 方法</h4>
<p>Relief:Relevant Features是一种著名的过滤式特征选择方法。该方法设计了一个相关统计量来度量特征的重要性。<br>
该统计量是一个向量，其中每个分量都对应于一个初始特征。特征子集的重要性则是由该子集中每个特征所对应的相关统计量分量之和来决定的。<br>
最终只需要指定一个阈值 k，然后选择比 k 大的相关统计量分量所对应的特征即可。<br>
也可以指定特征个数 m ，然后选择相关统计量分量最大的 m 个特征。<br>
Relief 是为二分类问题设计的，其拓展变体 Relief-F 可以处理多分类问题。</p>
<h4 id="方差选择法">方差选择法</h4>
<p>使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。</p>
<h4 id="相关系数法">相关系数法</h4>
<p>使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。</p>
<h4 id="卡方检验">卡方检验</h4>
<p>经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距，构建统计量：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>χ</mi><mn>2</mn></msup><mo>=</mo><mo>∑</mo><mfrac><msup><mrow><mo fence="true">(</mo><mi>A</mi><mo>−</mo><mi>E</mi><mo fence="true">)</mo></mrow><mn>2</mn></msup><mi>E</mi></mfrac></mrow><annotation encoding="application/x-tex">\chi ^2=\sum{\frac{\left( A-E \right) ^2}{E}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0585479999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">χ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.317008em;vertical-align:-0.686em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.631008em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p>
<p>不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。</p>
<h4 id="互信息法">互信息法</h4>
<p>经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mrow><mo fence="true">(</mo><mi>X</mi><mo separator="true">;</mo><mi>Y</mi><mo fence="true">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mrow><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mrow><mi>p</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>p</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow></mrow><mrow><mi>p</mi><mrow><mo fence="true">(</mo><mi>x</mi><mo fence="true">)</mo></mrow><mi>p</mi><mrow><mo fence="true">(</mo><mi>y</mi><mo fence="true">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">I\left( X;Y \right) =\sum_{x\in X}{\sum_{y\in Y}{p\left( x,y \right) \log \frac{p\left( x,y \right)}{p\left( x \right) p\left( y \right)}}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.857444em;vertical-align:-1.430444em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.07847em;">X</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.430444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault">x</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></span></p>
<p>为了处理定量数据，最大信息系数法被提出。</p>
<h3 id="包裹式选择">包裹式选择</h3>
<p>相比于过滤式特征选择不考虑后续学习器，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价原则。其目的就是为给定学习器选择最有利于其性能、量身定做的特征子集。</p>
<p>优点是直接针对特定学习器进行优化，考虑到特征之间的关联性，因此通常包裹式特征选择比过滤式特征选择能训练得到一个更好性能的学习器，<br>
缺点是由于特征选择过程需要多次训练学习器，故计算开销要比过滤式特征选择要大得多。</p>
<p>LVW:Las Vegas Wrapper是一个典型的包裹式特征选择方法。它是Las Vegas method 框架下使用随机策略来进行子集搜索，并以最终分类器的误差作为特征子集的评价标准。<br>
由于 LVW 算法中每次特征子集评价都需要训练学习器，计算开销很大，因此它会设计一个停止条件控制参数 T。<br>
但是如果初始特征数量很多、T 设置较大、以及每一轮训练的时间较长， 则很可能算法运行很长时间都不会停止。即：如果有运行时间限制，则有可能给不出解。</p>
<p>递归特征消除法：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。</p>
<h3 id="嵌入式选择">嵌入式选择</h3>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。<br>
嵌入式特征选择是将特征选择与学习器训练过程融为一体，两者在同一个优化过程中完成的。即学习器训练过程中自动进行了特征选择。</p>
<p>常用的方法包括：</p>
<p>利用正则化，如<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_1, L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；<br>
使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。<br>
引入 L_1 范数除了降低过拟合风险之外，还有一个好处：它求得的 w 会有较多的分量为零。即：它更容易获得稀疏解。<br>
于是基于 L_1 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。</p>
<p>常见的嵌入式选择模型：</p>
<ul>
<li>在 Lasso 中，λ 参数控制了稀疏性：</li>
<li>如果 λ 越小，则稀疏性越小，被选择的特征越多；</li>
<li>相反 λ 越大，则稀疏性越大，被选择的特征越少；</li>
<li>在 SVM 和 逻辑回归中，参数 C 控制了稀疏性：</li>
<li>如果 C 越小，则稀疏性越大，被选择的特征越少；</li>
<li>如果 C 越大， 则稀疏性越小，被选择的特征越多。</li>
</ul>
<blockquote>
<p>本文转载自<br>
https://blog.csdn.net/lc013/article/details/87898873<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程——异常值处理]]></title>
        <id>https://adiaosdu.github.io/post/PP-qqi0RB/</id>
        <link href="https://adiaosdu.github.io/post/PP-qqi0RB/">
        </link>
        <updated>2022-03-15T08:29:56.000Z</updated>
        <content type="html"><![CDATA[<h2 id="基本的处理方法">基本的处理方法</h2>
<ul>
<li>删除含有异常值的记录：直接将含有异常值的记录删除；</li>
<li>视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；</li>
<li>平均值修正：可用前后两个观测值的平均值修正该异常值；</li>
<li>不处理：直接在具有异常值的数据集上进行数据挖掘；</li>
</ul>
<p>将含有异常值的记录直接删除的方法简单易行，但缺点也很明显，在观测值很少的情况下，这种删除会造成样本量不足，可能会改变变量的原有分布，从而造成分析结果的不准确。视为缺失值处理的好处是可以利用现有变量的信息，对异常值（缺失值）进行填补。</p>
<p>在很多情况下，要先分析异常值出现的可能原因，在判断异常值是否应该舍弃，如果是正确的数据，可以直接在具有异常值的数据集上进行挖掘建模。</p>
<h2 id="处理类别不平衡问题">处理类别不平衡问题</h2>
<p>什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。</p>
<h3 id="扩充数据集">扩充数据集</h3>
<p>首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。人为调控以达到平衡</p>
<h3 id="尝试其他评价指标">尝试其他评价指标</h3>
<p>一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。</p>
<h4 id="accuracy-tptntpfptnfn">Accuracy = (TP+TN)/(TP+FP+TN+FN)</h4>
<h4 id="f1-score">F1 score</h4>
<ul>
<li>混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量</li>
<li>查准率(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，二分类时，预测为真的准确率为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>=</mo><mi>T</mi><mi>P</mi><mi mathvariant="normal">/</mi><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P=TP / (TP+FP)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span></li>
<li>召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，二分类时，预测为真的召回率为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>=</mo><mi>T</mi><mi>P</mi><mi mathvariant="normal">/</mi><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">R = TP / (TP+FN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord">/</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span></li>
<li>F1 得分(F1 Score)：查准率和召回率的加权平均，F1=2PR / (P+R)<br>
<img src="https://adiaosdu.github.io/post-images/1647334377167.png" alt="" loading="lazy"></li>
<li>第一类错误：实际上为真，但是拒绝了；第二类错误：实际上为假，但是接收了</li>
<li>推广：<br>
<img src="https://adiaosdu.github.io/post-images/1647334779366.png" alt="" loading="lazy"><br>
<img src="https://adiaosdu.github.io/post-images/1647334961055.png" alt="" loading="lazy"><br>
<img src="https://adiaosdu.github.io/post-images/1647334915590.png" alt="" loading="lazy"></li>
</ul>
<h4 id="多分类的情况">多分类的情况</h4>
<figure data-type="image" tabindex="1"><img src="https://adiaosdu.github.io/post-images/1647334885936.png" alt="" loading="lazy"></figure>
<h4 id="误警率-特异度">误警率、特异度</h4>
<figure data-type="image" tabindex="2"><img src="https://adiaosdu.github.io/post-images/1647335045931.png" alt="" loading="lazy"></figure>
<h4 id="p-r曲线">P-R曲线</h4>
<p>P-R曲线<br>
P-R曲线刻画查准率和查全率之间的关系，查准率指的是在所有预测为正例的数据中，真正例所占的比例，查全率是指预测为真正例的数据占所有正例数据的比例。<br>
即：查准率P=TP／(TP + FP) 查全率=TP／（TP+FN）<br>
查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低，例如，若希望将好瓜尽可能多选出来，则可通过增加选瓜的数量来实现，如果希望将所有的西瓜都选上，那么所有的好瓜必然都被选上了，但这样查准率就会较低；若希望选出的瓜中好瓜比例尽可能高，则可只挑选最有把握的瓜，但这样就难免会漏掉不少好瓜，使得查全率较低。</p>
<p>在很多情况下，我们可以根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，排在后面的是学习器认为最不可能是正例的样本，按此顺序逐个把样本作为正例进行预测，则每次可计算当前的查全率和查准率，以查准率为y轴，以查全率为x轴，可以画出下面的P-R曲线。<br>
<img src="https://adiaosdu.github.io/post-images/1647335323132.png" alt="" loading="lazy"><br>
如果一个学习器的P-R曲线被另一个学习器的P-R曲线完全包住，则可断言后者的性能优于前者，例如上面的A和B优于学习器C，但是A和B的性能无法直接判断，但我们往往仍希望把学习器A和学习器B进行一个比较，我们可以根据曲线下方的面积大小来进行比较，但更常用的是平衡点或者是F1值。平衡点（BEP）是查准率=查全率时的取值，如果这个值较大，则说明学习器的性能较好。同样，F1值越大，我们可以认为该学习器的性能较好。</p>
<h4 id="roc曲线">ROC曲线</h4>
<figure data-type="image" tabindex="3"><img src="https://adiaosdu.github.io/post-images/1647335428910.png" alt="" loading="lazy"></figure>
<ul>
<li>ROC曲线主要是用于X对Y的预测准确率情况。最初ROC曲线是运用在军事上，现在更多应用在医学领域，判断某种因素对于某种疾病的诊断是否有诊断价值。</li>
<li>横坐标：假阳性（误报率），即在所有判断为真的样本中，判断错误的占比</li>
<li>纵坐标：真阳性（敏感度/召回率），在所有真样本中，判断正确的比例</li>
</ul>
<p>如果一个模型，在误报率很低的数据中，能有一个较高的召回率，那就是较好的模型</p>
<h3 id="重采样">重采样</h3>
<p>对样本数量较多的类欠采样，对样本数量较少的类过采样</p>
<h3 id="人工辅助生成数据">人工辅助生成数据</h3>
<blockquote>
<p>参考资料：<br>
https://zhuanlan.zhihu.com/p/56557301<br>
https://www.jianshu.com/p/75a163a17fb5</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[离群点检测——孤立森林算法]]></title>
        <id>https://adiaosdu.github.io/post/TCUMQ2Tep/</id>
        <link href="https://adiaosdu.github.io/post/TCUMQ2Tep/">
        </link>
        <updated>2022-03-15T01:36:21.000Z</updated>
        <content type="html"><![CDATA[<h2 id="简介">简介</h2>
<p>工作的过程中经常会遇到这样一个问题，在构建模型训练数据时，我们很难保证训练数据的纯净度，数据中往往会参杂很多被错误标记噪声数据，而数据的质量决定了最终模型性能的好坏。如果进行人工二次标记，成本会很高，我们希望能使用一种无监督算法帮我们做这件事，异常检测算法可以在一定程度上解决这个问题。</p>
<p>异常检测分为 离群点检测(outlier detection) 以及 奇异值检测(novelty detection) 两种.</p>
<ul>
<li>离群点检测：适用于训练数据中包含异常值的情况，例如上述所提及的情况。离群点检测模型会尝试拟合训练数据最集中的区域，而忽略异常数据。</li>
<li>奇异值检测：适用于训练数据不受异常值的污染，目标是去检测新样本是否是异常值。 在这种情况下，异常值也被称为奇异点。<br>
孤立森林 (Isolation Forest, iForest)是一个基于Ensemble的快速离群点检测方法，具有线性时间复杂度和高精准度，是符合大数据处理要求的State-of-the-art算法。由南京大学周志华教授等人于2008年首次提出，之后又于2012年提出了改进版本。适用于连续数据(Continuous numerical data)的异常检测，与其他异常检测算法通过距离、密度等量化指标来刻画样本间的疏离程度不同，孤立森林算法通过对样本点的孤立来检测异常值。具体来说，该算法利用一种名为孤立树iTree的二叉搜索树结构来孤立样本。由于异常值的数量较少且与大部分样本的疏离性，因此，异常值会被更早的孤立出来，也即异常值会距离iTree的根节点更近，而正常值则会距离根节点有更远的距离。此外，相较于LOF，K-means等传统算法，孤立森林算法对高纬数据有较好的鲁棒性。其可以用于网络安全中的攻击检测，金融交易欺诈检测，疾病侦测，和噪声数据过滤等。</li>
</ul>
<p>举个例子：<br>
对于如何查找哪些点是否容易被孤立，iForest使用了一套非常高效的策略。假设我们用一个随机超平面来切割数据空间, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。上图里面黑色的点就很容易被切几次就停到一个子空间，而白色点聚集的地方可以切很多次才停止。</p>
<h2 id="算法">算法</h2>
<p>怎么来切这个数据空间是iForest的设计核心思想，本文仅介绍最基本的方法。由于切割是随机的，所以需要用Ensemble的方法来得到一个收敛值（蒙特卡洛方法），即反复从头开始切，然后平均每次切的结果。iForest 由 T 个 iTree 组成，每个 iTree 是一个二叉树结构。该算法大致可以分为两个阶段，第一个阶段我们需要训练出 T 颗孤立树，组成孤立森林。随后我们将每个样本点带入森林中的每棵孤立树，计算平均高度，之后再计算每个样本点的异常值分数。</p>
<h3 id="第一阶段">第一阶段：</h3>
<ul>
<li>从训练数据中随机选择Ψ个点样本点作为样本子集，放入树的根节点。</li>
<li>随机指定一个维度（特征），在当前节点数据中随机产生一个切割点 p（切割点产生于当前节点数据中指定维度的最大值和最小值之间）。</li>
<li>以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于 p 的数据放在当前节点的左子节点，把大于等于 p 的数据放在当前节点的右子节点。</li>
<li>在子节点中递归步骤(2)和(3)，不断构造新的子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度。</li>
<li>循环(1)至(4)，直至生成 T 个孤立树iTree。</li>
</ul>
<h3 id="第二阶段">第二阶段：</h3>
<p>获得T个iTree之后，iForest训练就结束，然后我们可以用生成的iForest来评估测试数据了。对于每一个数据点 x_i，令其遍历每一颗孤立树iTree，计算点 x_i 在森林中的平均高度 h(x_i) ，对所有点的平均高度做归一化处理。异常值分数的计算公式如下所示：<br>
<img src="https://adiaosdu.github.io/post-images/1647308422176.png" alt="" loading="lazy"></p>
<p>示例：</p>
<pre><code class="language-python">&gt;&gt;&gt; from sklearn.ensemble import IsolationForest
&gt;&gt;&gt; X = [[-1.1], [0.3], [0.5], [100]]
&gt;&gt;&gt; clf = IsolationForest(random_state=0).fit(X)
&gt;&gt;&gt; clf.predict([[0.1], [0], [90]])
array([ 1,  1, -1])
</code></pre>
<h2 id="补充">补充：</h2>
<p>iForest具有线性时间复杂度。因为是Ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。</p>
<p>iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度(irrelevant attributes)，影响树的构建。对这类数据，建议使用子空间异常检测(Subspace Anomaly Detection)技术。此外，切割平面默认是axis-parallel的，也可以随机生成各种角度的切割平面，详见“On Detecting Clustered Anomalies Using SCiForest”。</p>
<p>iForest仅对Global Anomaly敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点(Local Anomaly)。目前已有改进方法发表于PAKDD，详见“Improving iForest with Relative Mass”。</p>
<p>iForest推动了重心估计(Mass Estimation)理论发展，目前在分类聚类和异常检测中都取得显著效果，发表于各大顶级数据挖掘会议和期刊(如SIGKDD，ICDM，ECML)。</p>
<blockquote>
<p>本文转载自<br>
https://www.cnblogs.com/guoyaohua/p/isolation_forest.html<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程——降维、特征提取]]></title>
        <id>https://adiaosdu.github.io/post/ozKHLQfbm/</id>
        <link href="https://adiaosdu.github.io/post/ozKHLQfbm/">
        </link>
        <updated>2022-03-14T04:31:17.000Z</updated>
        <content type="html"><![CDATA[<p>特征提取一般是在特征选择之前，它提取的对象是原始数据，目的就是自动地构建新的特征，将原始数据转换为一组具有明显物理意义（比如 Gabor、几何特征、纹理特征）或者统计意义的特征。</p>
<p>一般常用的方法包括降维（PCA、ICA、LDA等）、图像方面的SIFT、Gabor、HOG等、文本方面的词袋模型、词嵌入模型等，这里简单介绍这几种方法的一些基本概念。</p>
<h3 id="降维">降维</h3>
<h4 id="pcaprincipal-component-analysis主成分分析">PCA(Principal Component Analysis，主成分分析)</h4>
<p>PCA 是降维最经典的方法，它旨在是找到数据中的主成分，并利用这些主成分来表征原始数据，从而达到降维的目的。<br>
PCA 的思想是通过坐标轴转换，寻找数据分布的最优子空间。<br>
比如，在三维空间中有一系列数据点，它们分布在过原点的平面上，如果采用自然坐标系的 x，y，z 三个轴表示数据，需要三个维度，但实际上这些数据点都在同一个二维平面上，如果我们可以通过坐标轴转换使得数据所在平面和 x，y 平面重合，我们就可以通过新的 x'、y'轴来表示原始数据，并且没有任何损失，这就完成了降维的目的，而且这两个新的轴就是我们需要找的主成分。</p>
<p>因此，PCA 的解法一般分为以下几个步骤：</p>
<ul>
<li>对样本数据进行中心化处理；</li>
<li>求样本协方差矩阵；</li>
<li>对协方差矩阵进行特征值分解，将特征值从大到小排列；</li>
<li>取特征值前 n 个最大的对应的特征向量 W1, W2, ..., Wn ，这样将原来 m 维的样本降低到 n 维。</li>
</ul>
<p>通过 PCA ，就可以将方差较小的特征给抛弃，这里，特征向量可以理解为坐标转换中新坐标轴的方向，特征值表示在对应特征向量上的方差，特征值越大，方差越大，信息量也就越大。这也是为什么选择前 n 个最大的特征值对应的特征向量，因为这些特征包含更多重要的信息。</p>
<p>PCA 是一种线性降维方法，这也是它的一个局限性。不过也有很多解决方法，比如采用核映射对 PCA 进行拓展得到核主成分分析(KPCA)，或者是采用流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些 PCA 效果不好的复杂数据集进行非线性降维操作。</p>
<h4 id="ldalinear-discriminant-analysis线性判别分析">LDA(Linear Discriminant Analysis，线性判别分析)</h4>
<p>LDA 是一种有监督学习算法，相比较 PCA，它考虑到数据的类别信息，而 PCA 没有考虑，只是将数据映射到方差比较大的方向上而已。<br>
因为考虑数据类别信息，所以 LDA 的目的不仅仅是降维，还需要找到一个投影方向，使得投影后的样本尽可能按照原始类别分开，即寻找一个可以最大化类间距离以及最小化类内距离的方向。</p>
<p>LDA 的优点如下：</p>
<ul>
<li>相比较 PCA，LDA 更加擅长处理带有类别信息的数据；<br>
线性模型对噪声的鲁棒性比较好，LDA 是一种有效的降维方法。<br>
相应的，也有如下缺点：</li>
<li>LDA 对数据的分布做出了很强的假设，比如每个类别数据都是高斯分布、各个类的协方差相等。这些假设在实际中不一定完全满足。</li>
<li>LDA 模型简单，表达能力有一定局限性。但这可以通过引入核函数拓展 LDA 来处理分布比较复杂的数据。</li>
</ul>
<h4 id="icaindependent-component-analysis独立成分分析">ICA(Independent Component Analysis，独立成分分析)</h4>
<p>PCA特征转换降维，提取的是不相关的部分，ICA独立成分分析，获得的是相互独立的属性。ICA算法本质寻找一个线性变换 z = Wx，使得 z 的各个特征分量之间的独立性最大。<br>
通常先采用 PCA 对数据进行降维，然后再用 ICA 来从多个维度分离出有用数据。PCA 是 ICA 的数据预处理方法。</p>
<h3 id="图像特征提取">图像特征提取</h3>
<p>图像的特征提取，在深度学习火起来之前，是有很多传统的特征提取方法，比较常见的包括以下几种。</p>
<h4 id="sift-特征">SIFT 特征</h4>
<p>SIFT 是图像特征提取中非常广泛应用的特征。它包含以下几种优点：</p>
<ul>
<li>具有旋转、尺度、平移、视角及亮度不变性，有利于对目标特征信息进行有效表达；</li>
<li>SIFT 特征对参数调整鲁棒性好，可以根据场景需要调整适宜的特征点数量进行特征描述，以便进行特征分析。</li>
</ul>
<p>SIFT 对图像局部特征点的提取主要包括四个步骤：</p>
<ul>
<li>疑似特征点检测</li>
<li>去除伪特征点</li>
<li>特征点梯度与方向匹配</li>
<li>特征描述向量的生成<br>
SIFT 的缺点是不借助硬件加速或者专门的图像处理器很难实现。</li>
</ul>
<h4 id="surf-特征">SURF 特征</h4>
<p>SURF 特征是对 SIFT 算法的改进，降低了时间复杂度，并且提高了鲁棒性。</p>
<p>它主要是简化了 SIFT 的一些运算，如将 SIFT 中的高斯二阶微分的模型进行了简化，使得卷积平滑操作仅需要转换成加减运算。并且最终生成的特征向量维度从 128 维减少为 64 维。</p>
<h4 id="hog-特征">HOG 特征</h4>
<p>方向梯度直方图(HOG)特征是 2005 年针对行人检测问题提出的直方图特征，它通过计算和统计图像局部区域的梯度方向直方图来实现特征描述。</p>
<p>HOG 特征提取步骤如下：</p>
<p>归一化处理。先将图像转为灰度图像，再利用伽马校正实现。这一步骤是为了提高图像特征描述对光照及环境变化的鲁棒性，降低图像局部的阴影、局部曝光过多和纹理失真，尽可能抵制噪声干扰；</p>
<ul>
<li>计算图像梯度；</li>
<li>统计梯度方向；</li>
<li>特征向量归一化；为克服光照不均匀变化及前景与背景的对比差异，需要对块内的特征向量进行归一化处理。</li>
<li>生成特征向量。</li>
</ul>
<h3 id="lbp-特征">LBP 特征</h3>
<p>局部二值模式（LBP）是一种描述图像局部纹理的特征算子，它具有旋转不变性和灰度不变性的优点。</p>
<p>LBP 特征描述的是一种灰度范围内的图像处理操作技术，针对的是输入为 8 位或者 16 位的灰度图像。</p>
<p>LBP 特征通过对窗口中心点与邻域点的关系进行比较，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了复杂场景下（光照变换）特征描述问题。</p>
<p>根据窗口领域的不同分为两种，经典 LBP 和圆形 LBP。前者的窗口是 3×3 的正方形窗口，后者将窗口从正方形拓展为任意圆形领域。</p>
<p>当然上述特征都是比较传统的图像特征提取方法了，现在图像基本都直接利用 CNN（卷积神经网络）来进行特征提取以及分类。</p>
<h3 id="文本特征提取">文本特征提取</h3>
<h4 id="词袋模型">词袋模型</h4>
<p>最基础的文本表示模型是词袋模型。</p>
<p>具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量的每一个维度代表一个单词，而该维度的权重反映了该单词在原来文章中的重要程度。</p>
<p>通常采用 TF-IDF 计算权重，公式为 $$TF-IDF(t, d) = TF(t,d) × IDF(t)$$</p>
<p>其中 TF(t, d) 表示单词 t 在文档 d 中出现的频率，IDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，其表示为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi mathvariant="normal">文</mi><mi mathvariant="normal">章</mi><mi mathvariant="normal">总</mi><mi mathvariant="normal">数</mi></mrow><mrow><mi mathvariant="normal">包</mi><mi mathvariant="normal">含</mi><mi mathvariant="normal">单</mi><mi mathvariant="normal">词</mi><mi>t</mi><mi mathvariant="normal">的</mi><mi mathvariant="normal">文</mi><mi mathvariant="normal">章</mi><mi mathvariant="normal">总</mi><mi mathvariant="normal">数</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mi mathvariant="normal">​</mi></mrow><annotation encoding="application/x-tex">IDF(t)=log
\frac{文章总数}{包含单词t的文章总数+1}
​</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.46377em;vertical-align:-0.7693300000000001em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.677em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord cjk_fallback">包</span><span class="mord cjk_fallback">含</span><span class="mord cjk_fallback">单</span><span class="mord cjk_fallback">词</span><span class="mord mathdefault">t</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">文</span><span class="mord cjk_fallback">章</span><span class="mord cjk_fallback">总</span><span class="mord cjk_fallback">数</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord cjk_fallback">文</span><span class="mord cjk_fallback">章</span><span class="mord cjk_fallback">总</span><span class="mord cjk_fallback">数</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">​</span></span></span></span></span></p>
<p>直观的解释就是，如果这个单词在多篇文章都出现过，那么它很可能是比较通用的词汇，对于区分文章的贡献比较小，自然其权重也就比较小，即 IDF(t) 会比较小。</p>
<h4 id="n-gram-模型">N-gram 模型</h4>
<p>词袋模型是以单词为单位进行划分，但有时候进行单词级别划分并不是很好的做法，毕竟有的单词组合起来才是其要表达的含义。因此可以将连续出现的 n 个词 (n &lt;= N) 组成的词组(N-gram)作为一个单独的特征放到向量表示中，构成了 N-gram 模型。</p>
<p>另外，同一个词可能会有多种词性变化，但却具有相同含义，所以实际应用中还会对单词进行词干抽取(Word Stemming)处理，即将不同词性的单词统一为同一词干的形式。</p>
<h4 id="词嵌入模型">词嵌入模型</h4>
<p>词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常 K=50~300 维）上的一个稠密向量（Dense Vector）。</p>
<p>常用的词嵌入模型是Word2Vec。它是一种底层的神经网络模型，有两种网络结构，分别是 CBOW(Continues Bag of Words) 和 Skip-gram。</p>
<p>CBOW 是根据上下文出现的词语预测当前词的生成概率；Skip-gram 是根据当前词来预测上下文中各个词的生成概率。</p>
<p>词嵌入模型是将每个词都映射成一个 K 维的向量，如果一篇文档有 N 个单词，那么每篇文档就可以用一个 N×K 的矩阵进行表示，但这种表示过于底层。实际应用中，如果直接将该矩阵作为原文本的特征表示输入到模型中训练，通常很难得到满意的结果，一般还需要对该矩阵进行处理，提取和构造更高层的特征。</p>
<p>深度学习模型的出现正好提供了一种自动进行特征工程的方法，它的每个隐含层都相当于不同抽象层次的特征。卷积神经网络(CNN)和循环神经网络(RNN)在文本表示中都取得了很好的效果，这是因为它们可以很好地对文本进行建模，抽取出一些高层的语义特征。</p>
<h3 id="特征提取和特征选择的区别">特征提取和特征选择的区别</h3>
<p>特征提取与特征选择都是为了从原始特征中找出最有效的特征。</p>
<p>它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；<br>
而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。</p>
<p>两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p>
<h3 id="特征构建">特征构建</h3>
<p>特征构建是指从原始数据中人工的构建新的特征。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。</p>
<p>特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用混合属性或者组合属性来创建新的特征，或是分解或切分原有的特征来创建新的特征。</p>
<p>特征构建非常需要相关的领域知识或者丰富的实践经验才能很好构建出更好的有用的新特征，相比于特征提取，特征提取是通过一些现成的特征提取方法来将原始数据进行特征转换，而特征构建就需要我们自己人为的手工构建特征，比如组合两个特征，或者分解一个特征为多个新的特征。</p>
<blockquote>
<p>本文转载自<br>
https://blog.csdn.net/lc013/article/details/87898873<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程——特征缩放与特征编码]]></title>
        <id>https://adiaosdu.github.io/post/N8LC-5z1W/</id>
        <link href="https://adiaosdu.github.io/post/N8LC-5z1W/">
        </link>
        <updated>2022-03-14T04:29:34.000Z</updated>
        <content type="html"><![CDATA[<p>特征缩放主要分为两种方法，归一化和正则化。</p>
<h2 id="归一化">归一化</h2>
<p>归一化(Normalization)，也称为标准化，这里不仅仅是对特征，实际上对于原始数据也可以进行归一化处理，它是将特征（或者数据）都缩放到一个指定的大致相同的数值区间内。</p>
<h3 id="归一化的两个原因">归一化的两个原因：</h3>
<ul>
<li>某些算法要求样本数据或特征的数值具有零均值和单位方差；</li>
<li>为了消除样本数据或者特征之间的量纲影响，即消除数量级的影响。
<ul>
<li>数量级的差异将导致量级较大的属性占据主导地位。量级较大的属性会让椭圆的等高线压缩为直线，使得目标函数仅依赖于该属性。</li>
<li>数量级的差异会导致迭代收敛速度减慢。原始的特征进行梯度下降时，每一步梯度的方向会偏离最小值（等高线中心点）的方向，迭代次数较多，且学习率必须非常小，否则非常容易引起宽幅震荡。但经过标准化后，每一步梯度的方向都几乎指向最小值（等高线中心点）的方向，迭代次数较少。</li>
<li>所有依赖于样本距离的算法对于数据的数量级都非常敏感。比如 KNN 算法需要计算距离当前样本最近的 k 个样本，当属性的量级不同，选择的最近的 k 个样本也会不同。</li>
</ul>
</li>
</ul>
<p>常用的两种归一化方法：</p>
<ul>
<li>对原始数据进行线性变换，使得结果映射到[0,1]的范围，实现对原始数据的等比缩放。</li>
<li>将原始数据映射到均值为 0，标准差为 1 的分布上。</li>
</ul>
<p>如果数据集分为训练集、验证集、测试集，那么三个数据集都采用相同的归一化参数，数值都是通过训练集计算得到，即上述两种方法中分别需要的数据最大值、最小值，方差和均值都是通过训练集计算得到（这个做法类似于深度学习中批归一化，BN的实现做法）。<br>
归一化不是万能的，实际应用中，通过梯度下降法求解的模型是需要归一化的，这包括线性回归、逻辑回归、支持向量机、神经网络等模型。但决策树模型不需要，以 C4.5 算法为例，决策树在分裂结点时候主要依据数据集 D 关于特征 x 的信息增益比，而信息增益比和特征是否经过归一化是无关的，归一化不会改变样本在特征 x 上的信息增益比。</p>
<h3 id="正则化">正则化</h3>
<p>正则化是将样本或者特征的某个范数（如 L1、L2 范数）缩放到单位 1。正则化后的结果是：每个属性值除以其 Lp 范数。可以理解为该样本点到原点的距离。</p>
<ul>
<li>正则化的过程是针对单个样本的，对每个样本将它缩放到单位范数。</li>
<li>归一化是针对单个属性的，需要用到所有样本在该属性上的值。</li>
</ul>
<h2 id="特征编码">特征编码</h2>
<h3 id="序号编码ordinal-encoding">序号编码(Ordinal Encoding)</h3>
<p>定义：序号编码一般用于处理类别间具有大小关系的数据。<br>
比如成绩，可以分为高、中、低三个档次，并且存在“高&gt;中&gt;低”的大小关系，那么序号编码可以对这三个档次进行如下编码：高表示为 3，中表示为 2，低表示为 1，这样转换后依然保留了大小关系。</p>
<h3 id="独热编码one-hot-encoding">独热编码(One-hot Encoding)</h3>
<p>定义：独热编码通常用于处理类别间不具有大小关系的特征。<br>
独热编码是采用 N 位状态位来对 N 个可能的取值进行编码。比如血型，一共有 4 个取值（A、B、AB 以及 O 型），那么独热编码会将血型转换为一个 4 维稀疏向量，分别表示上述四种血型为：</p>
<ul>
<li>A型：(1,0,0,0)</li>
<li>B型：(0,1,0,0)</li>
<li>AB型：(0,0,1,0)</li>
<li>O型：(0,0,0,1)</li>
</ul>
<p>独热编码的优点有以下几个：</p>
<ul>
<li>能够处理非数值属性。比如血型、性别等</li>
<li>一定程度上扩充了特征。</li>
<li>编码后的向量是稀疏向量，只有一位是 1，其他都是 0，可以利用向量的稀疏来节省存储空间。</li>
<li>能够处理缺失值。当所有位都是 0，表示发生了缺失。此时可以采用处理缺失值提到的高维映射方法，用第 N+1 位来表示缺失值。</li>
</ul>
<p>当然，独热编码也存在一些缺点：</p>
<ol>
<li>高维度特征会带来以下几个方面问题：
<ul>
<li>KNN 算法中，高维空间下两点之间的距离很难得到有效的衡量；</li>
<li>逻辑回归模型中，参数的数量会随着维度的增高而增加，导致模型复杂，出现过拟合问题；</li>
<li>通常只有部分维度是对分类、预测有帮助，需要借助特征选择来降低维度。<br>
2.决策树模型不推荐对离散特征进行独热编码，有以下两个主要原因：</li>
<li>产生样本切分不平衡问题，此时切分增益会非常小。</li>
<li>比如对血型做独热编码操作，那么对每个特征是否 A 型、是否 B 型、是否 AB 型、是否 O 型，会有少量样本是 1 ，大量样本是 0。这种划分的增益非常小，因为拆分之后：
<ul>
<li>较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略。</li>
<li>较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>本质是因为独热编码之后的特征的表达能力较差。该特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败。最终该特征得到的重要性会比实际值低。</p>
<h3 id="二进制编码binary-encoding">二进制编码(Binary Encoding)</h3>
<p>二进制编码主要分为两步：</p>
<ul>
<li>先采用序号编码给每个类别赋予一个类别 ID；</li>
<li>接着将类别 ID 对应的二进制编码作为结果。<br>
二进制编码本质上是利用二进制对类别 ID 进行哈希映射，最终得到 0/1 特征向量，并且特征维度小于独热编码，更加节省存储空间。</li>
</ul>
<h3 id="二元化">二元化</h3>
<p>定义：特征二元化就是将数值型的属性转换为布尔型的属性。通常用于假设属性取值分布是伯努利分布的情形。<br>
特征二元化的算法比较简单。对属性 j 指定一个阈值 m。<br>
如果样本在属性 j 上的值大于等于 m, 则二元化后为 1；<br>
如果样本在属性 j 上的值小于 m，则二元化为 0<br>
根据上述定义，m 是一个关键的超参数，它的取值需要结合模型和具体的任务来选择。</p>
<h3 id="离散化">离散化</h3>
<p>定义：顾名思义，离散化就是将连续的数值属性转换为离散的数值属性。需要采用“海量离散特征+简单模型”，还是“少量连续特征+复杂模型”的做法。</p>
<p>对于线性模型，通常使用“海量离散特征+简单模型”。<br>
优点：模型简单<br>
缺点：特征工程比较困难，但一旦有成功的经验就可以推广，并且可以很多人并行研究。</p>
<p>对于非线性模型（比如深度学习），通常使用“少量连续特征+复杂模型”。<br>
优点：不需要复杂的特征工程<br>
缺点：模型复杂</p>
<ol>
<li>离散化的常用方法是划分区间分桶：制定一系列的分桶边界，将数据划分到区间上</li>
<li>分桶的数量和边界通常需要人工指定。一般有两种方法：</li>
</ol>
<ul>
<li>
<p>根据业务领域的经验来指定。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：</p>
<ul>
<li>收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。</li>
<li>年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。</li>
<li>年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。</li>
<li>年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。</li>
<li>年收入超过 260 万元，则为分桶 4 。</li>
</ul>
</li>
<li>
<p>根据模型指定。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。</p>
</li>
</ul>
<ol start="3">
<li>选择分桶大小时，有一些经验指导：</li>
</ol>
<ul>
<li>分桶大小必须足够小，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。<br>
即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。</li>
<li>分桶大小必须足够大，使每个桶内都有足够的样本。<br>
如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。</li>
<li>每个桶内的样本尽量分布均匀。</li>
</ul>
<p>特性</p>
<ol>
<li>在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列 0/1 的离散特征。</li>
</ol>
<p>其优势有：</p>
<ul>
<li>离散化之后得到的稀疏向量，内积乘法运算速度更快，计算结果方便存储。</li>
<li>离散化之后的特征对于异常数据具有很强的鲁棒性。
<ul>
<li>如：销售额作为特征，当销售额在 [30,100) 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。</li>
</ul>
</li>
<li>逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于引入了非线性，提升模型的表达能力，增强拟合能力。</li>
<li>离散化之后可以进行特征交叉。假设有连续特征j ，离散化为 N 个 0/1 特征；连续特征 k，离散化为 M 个 0/1 特征，则分别进行离散化之后引入了 N+M 个特征。</li>
<li>假设离散化时，并不是独立进行离散化，而是特征 j,k 联合进行离散化，则可以得到 N*M 个组合特征。这会进一步引入非线性，提高模型表达能力。</li>
<li>离散化之后，模型会更稳定。<br>
如对销售额进行离散化，[30,100) 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。</li>
<li>但是处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理。</li>
</ul>
<ol start="2">
<li>特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险。<br>
能够对抗过拟合的原因：经过特征离散化之后，模型不再拟合特征的具体值，而是拟合特征的某个概念。因此能够对抗数据的扰动，更具有鲁棒性。<br>
另外它使得模型要拟合的值大幅度降低，也降低了模型的复杂度。</li>
</ol>
<blockquote>
<p>本文转载自<br>
https://zhuanlan.zhihu.com/p/56902262<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[迁移学习简介]]></title>
        <id>https://adiaosdu.github.io/post/X6D9gk5lG/</id>
        <link href="https://adiaosdu.github.io/post/X6D9gk5lG/">
        </link>
        <updated>2022-03-14T02:45:03.000Z</updated>
        <content type="html"><![CDATA[<p>迁移学习(transfer learning)通俗来讲，就是运用已有的知识来学习新的知识，核心是找到已有知识和新知识之间的相似性，用成语来说就是举一反三。由于直接对目标域从头开始学习成本太高，我们故而转向运用已有的相关知识来辅助尽快地学习新知识。比如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#；已经学会英语，就可以类比着来学习法语；等等。世间万事万物皆有共性，如何合理地找寻它们之间的相似性，进而利用这个桥梁来帮助学习新知识，是迁移学习的核心问题。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/jindongwang/transferlearning/master/png/tf.png" alt="图1不同位置、不同传感器的迁移标定。已知一个房间中A点的WiFi信号与相应的人体行为，如何标定另一个房间中C点的蓝牙信号？" loading="lazy"></figure>
<p>图1不同位置、不同传感器的迁移标定。已知一个房间中A点的WiFi信号与相应的人体行为，如何标定另一个房间中C点的蓝牙信号？</p>
<p>具体地，在迁移学习中，我们已有的知识叫做源域(source domain)，要学习的新知识叫目标域(target domain)。迁移学习研究如何把源域的知识迁移到目标域上。特别地，在机器学习领域中，迁移学习研究如何将已有模型应用到新的不同的、但是有一定关联的领域中。传统机器学习在应对数据的分布、维度，以及模型的输出变化等任务时，模型不够灵活、结果不够好，而迁移学习放松了这些假设。在数据分布、特征维度以及模型输出变化条件下，有机地利用源域中的知识来对目标域更好地建模。另外，在有标定数据缺乏的情况下，迁移学习可以很好地利用相关领域有标定的数据完成数据的标定。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/jindongwang/transferlearning/master/png/tf2.png" alt="" loading="lazy"></figure>
<p>图2 迁移学习与传统机器学习的不同。(a)传统机器学习对不同的学习任务建立不同的模型，(b)迁移学习利用源域中的数据将知识迁移到目标域，完成模型建立。插图来自：Sinno Jialin Pan and Qiang Yang, A survey on transfer learning. IEEE TKDE 2010.</p>
<p>迁移学习按照学习方式可以分为基于样本的迁移，基于特征的迁移，基于模型的迁移，以及基于关系的迁移。基于样本的迁移通过对源域中有标定样本的加权利用完成知识迁移；基于特征的迁移通过将源域和目标域映射到相同的空间（或者将其中之一映射到另一个的空间中）并最小化源域和目标域的距离来完成知识迁移；基于模型的迁移将源域和目标域的模型与样本结合起来调整模型的参数；基于关系的迁移则通过在源域中学习概念之间的关系，然后将其类比到目标域中，完成知识的迁移。</p>
<p>理论上，任何领域之间都可以做迁移学习。但是，如果源域和目标域之间相似度不够，迁移结果并不会理想，出现所谓的负迁移情况。比如，一个人会骑自行车，就可以类比学电动车；但是如果类比着学开汽车，那就有点天方夜谭了。如何找到相似度尽可能高的源域和目标域，是整个迁移过程最重要的前提。</p>
<p>迁移学习方面，代表人物有香港科技大学的Qiang Yang教授，南洋理工大学的Sinno Jialin Pan，以及第四范式的CEO戴文渊等。代表文献是Sinno Jialin Pan和Qiang Yang的A survey on transfer learning。</p>
<p>[参考资料]<br>
[1] Pan S J, Yang Q. A survey on transfer learning[J]. IEEE Transactions on knowledge and data engineering, 2010, 22(10): 1345-1359.<br>
[2] Introduction to Transfer Learning: http://jd92.wang/assets/files/l03_transferlearning.pdf。<br>
[3] Qiang Yang: http://www.cse.ust.hk/~qyang/.<br>
[4] Sinno Jialin Pan: http://www.ntu.edu.sg/home/sinnopan/.<br>
[5] Wenyuan Dai: https://scholar.google.com/citations?user=AGR9pP0AAAAJ&amp;hl=zh-CN.</p>
<blockquote>
<p>本文转载自<br>
https://github.com/jindongwang/transferlearning<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程——缺失值处理]]></title>
        <id>https://adiaosdu.github.io/post/jVZVV0BKV/</id>
        <link href="https://adiaosdu.github.io/post/jVZVV0BKV/">
        </link>
        <updated>2022-03-11T07:34:19.000Z</updated>
        <content type="html"><![CDATA[<p>为什么在机器学习中要做特征工程：不一定所有特征对于我们进行分类和回归都是有帮助的。进行特征工程有助于减少模型的参数，提升模型的泛化能力，最终提升模型的性能。</p>
<h2 id="数据预处理">数据预处理</h2>
<p>首先需要对数据进行预处理，一般常用的两种数据类型：</p>
<ul>
<li>结构化数据。结构化数据可以看作是关系型数据库的一张表，每列都有清晰的定义，包含了数值型和类别型两种基本类型；每一行数据表示一个样本的信息。</li>
<li>非结构化数据。主要是文本、图像、音频和视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每个数据的大小互不相同。</li>
</ul>
<p>这里主要介绍结构化数据和图像数据两种数据的数据预处理方法。</p>
<h3 id="处理缺失值">处理缺失值</h3>
<p>数据的缺失主要包括记录的缺失和记录中某个字段信息的缺失，两者都会造成分析结果的不准确。</p>
<ul>
<li>缺失值产生的原因
<ul>
<li>信息暂时无法获取，或者获取信息的代价太大。</li>
<li>信息被遗漏，人为的输入遗漏或者数据采集设备的遗漏。</li>
<li>属性不存在，在某些情况下，缺失值并不意味着数据有错误，对一些对象来说某些属性值是不存在的，例如未婚者的配偶姓名、儿童的固定收入等。</li>
</ul>
</li>
</ul>
<h4 id="缺失值的影响">缺失值的影响</h4>
<ul>
<li>数据挖掘建模将丢失大量的有用信息。</li>
<li>数据挖掘模型所表现出的不确定性更加显著，模型中蕴含的规律更难把握。</li>
<li>包含空值的数据会使建模过程陷入混乱，导致不可靠的输出。</li>
</ul>
<h4 id="缺失值的处理方法">缺失值的处理方法</h4>
<ul>
<li>直接使用含有缺失值的特征：当仅有少量样本缺失该特征的时候可以尝试使用；</li>
<li>删除含有缺失值的特征：这个方法一般适用于大多数样本都缺少该特征，且仅包含少量有效值是有效的；</li>
<li>插值补全缺失值：最常使用</li>
<li>均值/中位数/众数补全
<ul>
<li>如果样本属性的距离是可度量的，则使用该属性有效值的平均值来补全；</li>
<li>如果样本属性的距离不可度量，则可以采用众数或者中位数来补全。</li>
</ul>
</li>
<li>同类均值/中位数/众数补全
<ul>
<li>对样本进行分类后，根据同类其他样本该属性的均值补全缺失值，当然同第一种方法类似，如果均值不可行，可以尝试众数或者中位数等统计数据来补全。</li>
</ul>
</li>
<li>固定值补全：利用固定的数值补全缺失的属性值。</li>
<li>建模预测：利用机器学习方法，将缺失属性作为预测目标进行预测，具体为将样本根据是否缺少该属性分为训练集和测试集，然后采用如回归、决策树等机器学习算法训练模型，再利用训练得到的模型预测测试集中样本的该属性的数值。这个方法根本的缺陷是如果其他属性和缺失属性无关，则预测的结果毫无意义；但是若预测结果相当准确，则说明这个缺失属性是没必要纳入数据集中的；一般的情况是介于两者之间。</li>
<li>高维映射：将属性映射到高维空间，采用独热码编码（one-hot）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。（将missing单独作为一个新的属性）这种做法是最精确的做法，保留了所有的信息，也未添加任何额外信息，若预处理时把所有的变量都这样处理，会大大增加数据的维度。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值；缺点是计算量大大提升，且只有在样本量非常大的时候效果才好。</li>
<li>多重插补：多重插补认为待插补的值是随机的，实践上通常是估计出待插补的值，再加上不同的噪声，形成多组可选插补值，根据某种选择依据，选取最合适的插补值。</li>
<li>压缩感知和矩阵补全：压缩感知通过利用信号本身所具有的稀疏性，从部分观测样本中回复原信号。压缩感知分为感知测量和重构恢复两个阶段。</li>
<li>感知测量：此阶段对原始信号进行处理以获得稀疏样本表示。常用的手段是傅里叶变换、小波变换、字典学习、稀疏编码等</li>
<li>重构恢复：此阶段基于稀疏性从少量观测中恢复原信号。这是压缩感知的核心</li>
<li>手动补全<br>
除了手动补全方法，其他插值补全方法只是将未知值补以我们的主观估计值，不一定完全符合客观事实。在许多情况下，根据对所在领域的理解，手动对缺失值进行插补的效果会更好。但这种方法需要对问题领域有很高的认识和理解，要求比较高，如果缺失数据较多，会比较费时费力。</li>
<li>最近邻补全：寻找与该样本最接近的样本，使用其该属性数值来补全</li>
</ul>
<h3 id="图片数据扩充">图片数据扩充</h3>
<p>对于图片数据，最常遇到的问题就是训练数据不足的问题。一个模型所能获取的信息一般来源于两个方面，一个是训练数据包含的信息；另一个就是模型的形成过程中（包括构造、学习、推理等），人们提供的先验信息。<br>
而如果训练数据不足，那么模型可以获取的信息就比较少，需要提供更多的先验信息保证模型的效果。先验信息一般作用来两个方面，一是模型，如采用特定的内在结构（比如深度学习的不同网络结构）、条件假设或添加其他约束条件（深度学习中体现在损失函数加入不同正则项）；第二就是数据，即根据先验知识来调整、变换或者拓展训练数据，让其展现出更多的、更有用的信息。</p>
<p>对于图像数据，如果训练数据不足，导致的后果就是模型过拟合问题，即模型在训练样本上的效果不错，但在测试集上的泛化效果很糟糕。过拟合的解决方法可以分为两类：</p>
<ul>
<li>基于模型的方法：主要是采用降低过拟合风险的措施，如简化模型（从卷积神经网络变成逻辑回归算法）、添加约束项以缩小假设空间（如 L1、L2等正则化方法）、集成学习、Dropout方法（深度学习常用方法）等；</li>
<li>基于数据的方法：主要就是数据扩充(Data Augmentation)，即根据一些先验知识，在保持特点信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果。具体做法有多种，在保持图像类别不变的前提下，可以对每张图片做如下变换处理。
<ul>
<li>一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等，这些变换对应着同一个目标在不同角度的观察结果；</li>
<li>对图像中的元素添加噪声扰动，如椒盐噪声、高斯白噪声等；</li>
<li>颜色变换。比如在图像的 RGB 颜色空间进行主成分分析，得到 3 个主成分的特征向量p1,p2,p3以及对应的特征值λ1,λ2,λ3，然后在每个像素的 RGB 值上添加增量[p1,p2,p3]*[a1λ1,a2λ2,a3λ3]，其中a1,a2,a3都是均值为 0， 方差较小的高斯分布随机数；</li>
<li>改变图像的亮度、清晰度、对比度、锐度等。</li>
</ul>
</li>
</ul>
<p>上述数据扩充方法是在图像空间进行变换的，也可以选择先对图像进行特征提取，然后在图像的特征空间进行变换，利用一些通用的数据扩充或者上采样方法，例如 SMOTE(Synthetic Minority Over-sampling Technique)。<br>
此外，最近几年一直比较热门的 GAN，生成对抗网络，它的其中一个应用就是生成图片数据，也可以应用于数据扩充。</p>
<blockquote>
<p>本文大部分内容摘录自：<br>
https://blog.csdn.net/lc013/article/details/86939716<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[特征工程——异常值检测]]></title>
        <id>https://adiaosdu.github.io/post/1zXPS6Gnr/</id>
        <link href="https://adiaosdu.github.io/post/1zXPS6Gnr/">
        </link>
        <updated>2022-03-11T06:11:02.000Z</updated>
        <content type="html"><![CDATA[<p>异常值分析是检验数据是否有录入错误以及含有不合常理的数据。忽视异常值的存在是十分危险的，不加剔除地把异常值包括进数据的计算分析过程中，对结果会产生不良影响。异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也称为离群点，异常值分析也称为离群点分析。</p>
<h3 id="可视化方法">可视化方法：</h3>
<ul>
<li>低维数据可以直接考虑散点图，绘制出散点图</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mi>σ</mi></mrow><annotation encoding="application/x-tex">3\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span> 原则: 这个原则有个条件：数据需要服从正态分布。在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mi>σ</mi></mrow><annotation encoding="application/x-tex">3\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span> 原则下，异常值如超过 3 倍标准差，那么可以将其视为异常值。正负 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mi>σ</mi></mrow><annotation encoding="application/x-tex">3\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span> 的概率是 99.7%，那么距离平均值 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mi>σ</mi></mrow><annotation encoding="application/x-tex">3\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span> 之外的值出现的概率为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo>−</mo><mi>μ</mi><mi mathvariant="normal">∣</mi><mo>&gt;</mo><mn>3</mn><mi>σ</mi><mo>)</mo><mo>≤</mo><mn>0.003</mn></mrow><annotation encoding="application/x-tex">P(|x-\mu| &gt; 3\sigma) \leq 0.003</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">μ</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">0</span><span class="mord">3</span></span></span></span>，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。</li>
<li>箱线图
<ul>
<li>箱线图由五个数值点组成：最小值(min)，下四分位数(Q1)，中位数(median)，上四分位数(Q3)，最大值(max)。也可以加入平均值(mean)。</li>
<li>将离群点单独绘出。箱线图中的两极修改成最小观测值与最大观测值。这里有个经验，就是最大(最小)观测值设置为与四分位数值间距离为1.5个IQR(中间四分位数极差)。两极外的点为离群点。</li>
<li>用箱线图可以观察数据的分布、偏向、离群情况等。<br>
<img src="https://adiaosdu.github.io/post-images/1647331949468.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="基于模型预测">基于模型预测</h3>
<p>顾名思义，该方法会构建一个概率分布模型，并计算对象符合该模型的概率，将低概率的对象视为异常点。<br>
如果模型是簇的组合，则异常点是不在任何簇的对象；如果模型是回归，异常点是远离预测值的对象。</p>
<ul>
<li>优点：有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；</li>
<li>缺点：对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。</li>
</ul>
<h3 id="基于近邻度的离群点检测">基于近邻度的离群点检测</h3>
<p>一个对象的离群点得分由到它的 k-最近邻（KNN）的距离给定。为了增强鲁棒性，一般采用 k 个最近邻的平均距离。<br>
这里需要注意 k 值的取值会影响离群点得分，如果 k 太小，则少量的邻近离群点可能会导致较低的离群点得分；如果 k 太大，则点数少于 k 的簇中所有的对象可能都成了离群点。</p>
<ul>
<li>优点：简单;</li>
<li>缺点：
<ul>
<li>基于邻近度的方法需要 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>m</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(m^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 时间，大数据集不适用；</li>
<li>k 值的取值导致该方法对参数的选择也是敏感的；</li>
<li>不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。</li>
</ul>
</li>
</ul>
<h3 id="基于密度的离群点检测">基于密度的离群点检测</h3>
<p>一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。<br>
另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。</p>
<ul>
<li>优点给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；</li>
<li>缺点：与基于距离的方法一样，这些方法必然具有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>m</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(m^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的时间复杂度。对于低维数据使用特定的数据结构可以达到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>m</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>m</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(mlogm)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span> ；</li>
</ul>
<h3 id="基于密度的离群点检测-2">基于密度的离群点检测</h3>
<p>一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。<br>
另一种密度定义是使用 DBSCAN 聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离 d 内对象的个数。</p>
<ul>
<li>优点给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；</li>
<li>缺点：与基于距离的方法一样，这些方法必然具有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>m</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(m^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的时间复杂度。对于低维数据使用特定的数据结构可以达到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>m</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>m</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(mlogm)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mclose">)</span></span></span></span> ；</li>
</ul>
<h3 id="基于聚类的离群点检测">基于聚类的离群点检测</h3>
<p>一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。这也是 k-means 算法的缺点，对离群点敏感。</p>
<p>为了处理该问题，可以删除离群点后再次聚类，但是不保证产生最优结果。</p>
<ul>
<li>优点：
<ul>
<li>k-means 算法较为简单；</li>
<li>簇的定义通常是离群点的补集，因此可能同时发现簇和离群点；</li>
</ul>
</li>
<li>缺点：
<ul>
<li>产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；</li>
<li>聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。</li>
</ul>
</li>
</ul>
<h3 id="专门的离群点检测算法one-class-svm和isolation-forest等">专门的离群点检测算法：One Class SVM和Isolation Forest等</h3>
<blockquote>
<p>本文节选自<br>
https://zhuanlan.zhihu.com/p/56557301<br>
转载请注明原出处</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSL/TLS协议运行机制]]></title>
        <id>https://adiaosdu.github.io/post/sDe7ttOaF/</id>
        <link href="https://adiaosdu.github.io/post/sDe7ttOaF/">
        </link>
        <updated>2022-03-11T05:45:13.000Z</updated>
        <content type="html"><![CDATA[<p>本文转载并修改自阮一峰的网络日志：https://www.ruanyifeng.com/blog/2014/02/ssl_tls.html</p>
<h2 id="一-作用">一、作用</h2>
<p>SSL工作在应用层，用于保证HTTP通信的安全性。访问协议https<br>
不使用SSL/TLS的HTTP通信，就是不加密的通信。所有信息明文传播，带来了三大风险。</p>
<p>（1） 窃听风险（eavesdropping）：第三方可以获知通信内容。<br>
（2） 篡改风险（tampering）：第三方可以修改通信内容。<br>
（3） 冒充风险（pretending）：第三方可以冒充他人身份参与通信。</p>
<p>SSL/TLS协议是为了解决这三大风险而设计的，希望达到：<br>
（1） 所有信息都是加密传播，第三方无法窃听。<br>
（2） 具有校验机制，一旦被篡改，通信双方会立刻发现。<br>
（3） 配备身份证书，防止身份被冒充。</p>
<p>互联网是开放环境，通信双方都是未知身份，这为协议的设计带来了很大的难度。而且，协议还必须能够经受所有匪夷所思的攻击，这使得SSL/TLS协议变得异常复杂。</p>
<h2 id="二-历史">二、历史</h2>
<p>互联网加密通信协议的历史，几乎与互联网一样长。<br>
1994年，NetScape公司设计了SSL协议（Secure Sockets Layer）的1.0版，但是未发布。<br>
1995年，NetScape公司发布SSL 2.0版，很快发现有严重漏洞。<br>
1996年，SSL 3.0版问世，得到大规模应用。<br>
1999年，互联网标准化组织ISOC接替NetScape公司，发布了SSL的升级版TLS 1.0版。<br>
2006年和2008年，TLS进行了两次升级，分别为TLS 1.1版和TLS 1.2版。最新的变动是2011年TLS 1.2的修订版。</p>
<p>目前，应用最广泛的是TLS 1.0，接下来是SSL 3.0。但是，主流浏览器都已经实现了TLS 1.2的支持。<br>
TLS 1.0通常被标示为SSL 3.1，TLS 1.1为SSL 3.2，TLS 1.2为SSL 3.3。</p>
<h2 id="三-基本的运行过程">三、基本的运行过程</h2>
<p>SSL/TLS协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。</p>
<p>但是，这里有两个问题。</p>
<p>（1）如何保证公钥不被篡改？<br>
解决方法：将公钥放在数字证书中。只要证书是可信的，公钥就是可信的。</p>
<p>（2）公钥加密计算量太大，如何减少耗用的时间？<br>
解决方法：每一次对话（session），客户端和服务器端都生成一个&quot;对话密钥&quot;（session key），用它来加密信息。由于&quot;对话密钥&quot;是对称加密，所以运算速度非常快，而服务器公钥只用于加密&quot;对话密钥&quot;本身，这样就减少了加密运算的消耗时间。</p>
<p>因此，SSL/TLS协议的基本过程是这样的：</p>
<p>（1） 客户端向服务器端索要并验证公钥。<br>
（2） 双方协商生成&quot;对话密钥&quot;。<br>
（3） 双方采用&quot;对话密钥&quot;进行加密通信。</p>
<p>上面过程的前两步，又称为&quot;握手阶段&quot;（handshake）。</p>
<h2 id="四-握手阶段的详细过程">四、握手阶段的详细过程</h2>
<p>&quot;握手阶段&quot;涉及四次通信。需要注意的是，&quot;握手阶段&quot;的所有通信都是明文的。</p>
<h3 id="41-客户端发出请求clienthello">4.1 客户端发出请求（ClientHello）</h3>
<p>首先，客户端（通常是浏览器）先向服务器发出加密通信的请求，这被叫做ClientHello请求。<br>
在这一步，客户端主要向服务器提供以下信息。<br>
（1） 支持的协议版本，比如TLS 1.0版。<br>
（2） 一个客户端生成的随机数，稍后用于生成&quot;对话密钥&quot;。<br>
（3） 支持的加密方法，比如RSA公钥加密。<br>
（4） 支持的压缩方法。</p>
<p>这里需要注意的是，客户端发送的信息之中不包括服务器的域名。也就是说，理论上服务器只能包含一个网站，否则会分不清应该向客户端提供哪一个网站的数字证书。这就是为什么通常一台服务器只能有一张数字证书的原因。</p>
<p>对于虚拟主机的用户来说，这当然很不方便。2006年，TLS协议加入了一个Server Name Indication扩展，允许客户端向服务器提供它所请求的域名。</p>
<h3 id="42-服务器回应severhello">4.2 服务器回应（SeverHello）</h3>
<p>服务器收到客户端请求后，向客户端发出回应，这叫做SeverHello。服务器的回应包含以下内容。</p>
<p>（1） 确认使用的加密通信协议版本，比如TLS 1.0版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。<br>
（2） 一个服务器生成的随机数，稍后用于生成&quot;对话密钥&quot;。<br>
（3） 确认使用的加密方法，比如RSA公钥加密。<br>
（4） 服务器证书。</p>
<p>除了上面这些信息，如果服务器需要确认客户端的身份，就会再包含一项请求，要求客户端提供&quot;客户端证书&quot;。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。</p>
<h3 id="43-客户端回应">4.3 客户端回应</h3>
<p>客户端收到服务器回应以后，首先验证服务器证书。如果证书不是可信机构颁布、或者证书中的域名与实际域名不一致、或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。</p>
<p>如果证书没有问题，客户端就会从证书中取出服务器的公钥。然后，向服务器发送下面三项信息。<br>
（1） 一个随机数。该随机数用服务器公钥加密，防止被窃听。<br>
（2） 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。<br>
（3） 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验。</p>
<p>上面第一项的随机数，是整个握手阶段出现的第三个随机数，又称&quot;pre-master key&quot;。有了它以后，客户端和服务器就同时有了三个随机数，接着双方就用事先商定的加密方法，各自生成本次会话所用的同一把&quot;会话密钥&quot;。</p>
<p>对于RSA密钥交换算法来说，pre-master-key本身就是一个随机数，再加上hello消息中的随机数，三个随机数通过一个密钥导出器最终导出一个对称密钥。</p>
<p>pre master的存在在于SSL协议不信任每个主机都能产生完全随机的随机数，如果随机数不随机，那么pre master secret就有可能被猜出来，那么仅适用pre master secret作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器加上pre master secret三个随机数一同生成的密钥就不容易被猜出了，一个伪随机可能完全不随机，可是是三个伪随机就十分接近随机了，每增加一个自由度，随机性增加的可不是一。&quot;</p>
<p>此外，如果前一步，服务器要求客户端证书，客户端会在这一步发送证书及相关信息。</p>
<h3 id="44-服务器的最后回应">4.4 服务器的最后回应</h3>
<p>服务器收到客户端的第三个随机数pre-master key之后，计算生成本次会话所用的&quot;会话密钥&quot;。然后，向客户端最后发送下面信息。</p>
<p>（1）编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。<br>
（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供客户端校验。<br>
至此，整个握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的HTTP协议，只不过用&quot;会话密钥&quot;加密内容。</p>
<h2 id="总结ssl的四次握手">总结SSL的四次握手</h2>
<p>第一次握手：客户机发送访问请求和随机数1<br>
第二次握手：服务器回复自身证书和随机数2<br>
第三次握手：客户机验证证书，使用证书中的公钥加密一个随机数3并发送，随后用三个随机数生成会话密钥<br>
第四次握手：服务器用私钥解密随机数3，也生成会话密钥</p>
]]></content>
    </entry>
</feed>